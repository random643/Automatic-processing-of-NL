# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mlRW9q32EXn3B92Da0AdAzPhKQwzIdKc
"""

import csv
import re
from google.colab import drive
drive.mount('/content/drive')
import os

import regex as re
import os
import nltk
from nltk.corpus import stopwords
from google.colab import drive
import pandas as pd


nltk.download('stopwords')

assets_url = '/content/drive/MyDrive/lab1.16/result/'
assets_generated_url = '/content/drive/MyDrive/lab3/'
stops = set(stopwords.words('english'))

import nltk
from nltk.corpus import stopwords

nltk.download('punkt')
# скачиваем список стоп-слов
nltk.download('stopwords')

# загружаем список стоп-слов
stop_words = set(stopwords.words('english'))

token_freq_dict = dict()
file_names_contents_dict = dict()
S = 0

for address, dirs, files in os.walk(assets_url):
  for file_name in files:
    file_content = []
    with open(os.path.join(address, file_name), mode='r') as annotated_document_file:
      for sentence in annotated_document_file.read().split('\n\n'):
        for annotation in sentence.split('\n'):
          word_stem_lem = annotation.split('\t')
          if len(word_stem_lem) == 3:
            token = word_stem_lem[0]
            # Очистить полученные данные от знаков пунктуации. Можно использовать регулярное выражение: [^\P{P}-]+;
            if not re.match('[^\P{P}-]+', token):
              token = re.sub('[^\P{P}-]+', '', token) # Привести полученные данные к нижнему регистру;
              # Очистить полученные данные от стоп слов. Можно использовать nltk.corpus.stopwords;
              if not token in stops:
                # Словарь токенов с их частотами по всем данным
                token_freq_dict[token] = token_freq_dict.get(token, 0) + 1
                file_content.append(token)
                S += 1
    file_names_contents_dict[file_name] = file_content

import os

# Define the directory where you want to save the TSV file
output_directory = '/content/drive/MyDrive/lab3/'

# Ensure the directory exists; create it if it doesn't
os.makedirs(output_directory, exist_ok=True)

# Define the TSV file path within the directory
tsv_file_path = os.path.join(output_directory, 'file-content-dict.tsv')

# Create the TSV file and write the contents of the dictionary into it
with open(tsv_file_path, 'w') as tsvfile:
    for key, value in file_names_contents_dict.items():
        tsvfile.write(f"{key}\t{value}\n")

# Print a message to confirm that the file has been created
print(f"TSV file '{tsv_file_path}' has been created and populated with dictionary contents.")

# read file-content-dict, token_freq and term-doc-matrix from files
token_freq_dict_url = '/content/drive/MyDrive/lab3/'
file_names_contents_df = pd.read_table(token_freq_dict_url + 'file-content-dict.tsv', )

file_names_contents_df

import pandas as pd

# Convert the token_freq_dict to a DataFrame
token_freq_df = pd.DataFrame(list(token_freq_dict.items()), columns=['token', 'frequency'])

# Define the file path where you want to save the CSV file
csv_file_path = '/content/drive/MyDrive/lab3/token_freq_dict.csv'

# Save the DataFrame to a CSV file
token_freq_df.to_csv(csv_file_path, index=False)

# Print a message to confirm that the CSV file has been saved
print(f"Token frequency data has been saved to '{csv_file_path}'.")

token_freq_dict_url = '/content/drive/MyDrive/lab3/token_freq_dict.csv'
tokens_freq_df = pd.read_csv(token_freq_dict_url)

tokens_freq_df

import pandas as pd
import os

# Замените на ваш путь к данным
data_directory = '/content/drive/MyDrive/lab1.16/result/test/'

# Создаем пустую матрицу term-document
term_doc_matrix = pd.DataFrame(0, index=range(len(file_names_contents_dict)), columns=token_freq_dict.keys())

# Заполняем матрицу на основе данных о частотах токенов в документах
for idx, (file_name, file_content) in enumerate(file_names_contents_dict.items()):
    for token in file_content:
        if token in token_freq_dict:
            term_doc_matrix.at[idx, token] = file_content.count(token)

# term-document matrix term_doc_matrix
term_doc_matrix.to_csv('/content/drive/MyDrive/lab3/term-doc-matrix.csv')

term_doc_matrix_url = '/content/drive/MyDrive/lab3/term-doc-matrix.csv'
term_doc_matrix_df = pd.read_csv(term_doc_matrix_url)

term_doc_matrix_df

# 2. Реализовать один из базовых методов векторизации произвольного текста
# Разработать метод, позволяющий преобразовать произвольный текст в вектор значений tf-idf, с использованием словаря наиболее частых слов и матрицы "термин-документ", полученных ранее (на шаге 1);

import math

def get_TF_vec_of_doc(doc_text, available_tokens):
  doc_tokens = re.findall(r'[^\s.!?\-;:]+', doc_text)
  TF_vec = pd.DataFrame(data=0, index=[0], columns=available_tokens)
  for doc_token in doc_tokens:
    if doc_token in available_tokens:
      TF_vec[doc_token] += 1
  for token in available_tokens:
    TF_vec[token] = TF_vec[token]/len(available_tokens)
  return TF_vec

def getIDF_vec(term_doc_matrix, available_tokens):
  IDF_vec = pd.DataFrame(data=0, index=[0], columns=available_tokens.keys())
  for token in available_tokens:
    for index, row in term_doc_matrix.iterrows():
      if row[token] != 0:
        IDF_vec[token] += 1
  for token in available_tokens:
    if IDF_vec[token][0] != 0:
      IDF_vec[token] = math.log(term_doc_matrix.shape[0] / IDF_vec[token])
  return IDF_vec

get_TF_vec_of_doc('help me get a Cossack.Asian	Shares	Hit	Metals. I watch all the episodes of Dr. House in the world there is nothing else to watch', token_freq_dict.keys())

term_doc_matrix.head()

idf_dataframe = getIDF_vec(term_doc_matrix_df.head(2000), token_freq_dict)

idf_dataframe

# 3.
from gensim.models import Word2Vec
from gensim.test.utils import common_texts



data = []
for idf_dataframe in idf_dataframe:
  list_in_normal_form = idf_dataframe.strip('][').split(', ')
  clear_list = []
  for word in list_in_normal_form:
    word_clear = word[1:]
    word_clear = word_clear[:-1]
    clear_list.append(word_clear)
  data.append(clear_list)

data

w2v_model = Word2Vec(
    min_count=10,
    vector_size=100,
    negative=10,
    alpha=0.03,
    min_alpha=0.0007,
    sample=6e-5,
    sg=1)

w2v_model.build_vocab(data)

w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=5, report_delay=1)

w2v_model.save(assets_generated_url + 'word2vec.model')

w2v_model = Word2Vec.load(assets_generated_url + 'word2vec.model')

word_list = w2v_model.wv.index_to_key
print(word_list)

w2v_model.wv.get_vector('lowe')

# 4. С использованием библиотечной реализации метода подсчета косинусного расстояния между векторными представлениями текста,
# продемонстрировать на примерах, что для семантически близких слов модель генерирует вектора, для которых косинусное расстояние меньше, чем для семантически далеких токенов

# изменяется от 0 до 1 и чем больше - тем лсова более похожи
from scipy import spatial
def cosine_lib(a, b):
    return 1 - spatial.distance.cosine(a, b)

def cosine_similarity_of_vectors(vec_1, vec_2):
  min_vec_size = len(vec_1)
  if min_vec_size > len(vec_2):
    min_vec_size = len(vec_2)
  dot_product_of_vectors = 0
  vec_1_magnitude = 0
  vec_2_magnitude = 0
  for vec_param_index in range(0,min_vec_size):
    dot_product_of_vectors += vec_1[vec_param_index] * vec_2[vec_param_index]
    vec_1_magnitude += vec_1[vec_param_index] * vec_1[vec_param_index]
    vec_2_magnitude += vec_2[vec_param_index] * vec_2[vec_param_index]
  vec_1_magnitude = math.sqrt(vec_1_magnitude)
  vec_2_magnitude = math.sqrt(vec_2_magnitude)

  return dot_product_of_vectors / (vec_1_magnitude * vec_2_magnitude)

set_of_examples = {}
def analyze_word_set(w2v_model, set_of_examples):
    for example_word, tests in set_of_examples.items():
        print(f"Analyzing word - {example_word}")
        example_word_vec = w2v_model.wv.get_vector(example_word)

        for test_category, test_words in tests.items():
            print(f"\t{test_category} words scores:")
            for test_word in test_words:
                test_word_vec = w2v_model.wv.get_vector(test_word)
                my_cos_dist = cosine_similarity_of_vectors(example_word_vec, test_word_vec)
                lib_cos_dist = cosine_lib(example_word_vec, test_word_vec)
                print(f"\t\tDistance for {test_word}")
                print(f"\t\t\tmy  = {my_cos_dist}")
                print(f"\t\t\tlib = {lib_cos_dist}")
            print()

# Пример использования:
analyze_word_set(w2v_model, set_of_examples)

test_url = '/content/drive/MyDrive/lab1.1/ishodniki/test.csv'
train_url = '/content/drive/MyDrive/lab1.1/ishodniki/train.csv'

column_names = ["class", "title", "text"]
df_train = pd.read_csv(train_url, names=column_names)
df_test = pd.read_csv(test_url, names=column_names)

# 5. Применить какой-либо метод сокращения размерностей полученных одним из базовых способов векторизации, выбранным ранее (см. пункт 2), векторов
from sklearn.decomposition import PCA

vectors = []
df_test = pd.read_csv(test_url, names=column_names)

for index, row in df_test.head(50).iterrows():
  text = row['title'] + '. ' + row['text']
  df_vector = get_TF_vec_of_doc(text, token_freq_dict.keys())
  vector_as_list = df_vector.loc[0, :].values.flatten().tolist()
  vectors.append(vector_as_list)

pca = PCA(n_components=50)
pca_model = pca.fit(vectors)

import matplotlib.pyplot as plt

# Предположим, что у вас есть преобразованные данные после PCA
pca_result = pca_model.transform(vectors)

# визуализировать две компоненты PCA
for i in range(10):
    plt.figure(figsize=(8, 6))
    plt.scatter(pca_result[:, i], pca_result[:, i+1])
    plt.xlabel(f'Principal Component {i+1}')
    plt.ylabel(f'Principal Component {i+2}')
    plt.title(f'PCA Visualization (Component {i+1} vs. Component {i+2})')
    plt.show()

# 6. С использованием разработанного метода подсчета косинусного расстояния сравнить эффективность метода векторизации с использованием
#  нейронных сетей и эффективность базовых методов векторизации с последующим сокращением размерности.
# Сформулировать вывод о том, применение какого способа позволяет получить лучшие результаты на выбранном датасете.

pca_ini_vector = pca_model.transform(get_TF_vec_of_doc('lowe', token_freq_dict.keys()))[0]
pca_same_vector = pca_model.transform(get_TF_vec_of_doc('row', token_freq_dict.keys()))[0]
pca_diff_vector = pca_model.transform(get_TF_vec_of_doc('out', token_freq_dict.keys()))[0]

w2v_ini_vector = w2v_model.wv.get_vector('lowe')
w2v_same_vector = w2v_model.wv.get_vector('row')
w2v_diff_vector = w2v_model.wv.get_vector('out')

my_cos_dist_pca_same = cosine_similarity_of_vectors(pca_ini_vector, pca_same_vector)
my_cos_dist_pca_diff = cosine_similarity_of_vectors(pca_ini_vector, pca_diff_vector)

lib_cos_dist_pca_same = cosine_lib(pca_ini_vector, pca_same_vector)
lib_cos_dist_pca_diff = cosine_lib(pca_ini_vector, pca_diff_vector)


my_cos_dist_w2v_same = cosine_similarity_of_vectors(w2v_ini_vector, w2v_same_vector)
my_cos_dist_w2v_diff = cosine_similarity_of_vectors(w2v_ini_vector, w2v_diff_vector)

lib_cos_dist_w2v_same = cosine_lib(w2v_ini_vector, w2v_same_vector)
lib_cos_dist_w2v_diff = cosine_lib(w2v_ini_vector, w2v_diff_vector)

print('PCA')
print('\tMY cosine')
print(f"\t\tsame={my_cos_dist_pca_same}\t\tdiff={my_cos_dist_pca_diff}")
print('\tLIB cosine')
print(f"\t\tsame={lib_cos_dist_pca_same}\t\tdiff={lib_cos_dist_pca_diff}")

print()

print('W2V')
print('\tMY cosine')
print(f"\t\tsame={my_cos_dist_w2v_same}\t\tdiff={my_cos_dist_w2v_diff}")
print('\tLIB cosine')
print(f"\t\tsame={lib_cos_dist_w2v_same}\t\tdiff={lib_cos_dist_w2v_diff}")

# 7. Реализовать метод, осуществляющий векторизацию произвольного текста по следующему алгоритму
def vectorize_text(text, w2v_model, stops=None):
    if stops is None:
        stops = set()  # Если список стоп-слов не передан, создаем пустой

    token_vectors = []
    total_tokens = 0

    # Разбить текст на токены
    tokens = re.findall(r'[^\s.!?\-;:]+', text)

    for token in tokens:
        if token not in stops and token in w2v_model.wv:
            token_vector = w2v_model.wv.get_vector(token)
            token_vectors.append(token_vector)
            total_tokens += 1

    if total_tokens > 0:
        # Средний вектор всех токенов
        text_vector = np.mean(token_vectors, axis=0)
        return text_vector.tolist()
    else:
        return [0] * w2v_model.vector_size  # Вернуть нулевой вектор, если нет допустимых токенов

import re
import numpy as np

vectorize_text('The quick brown fox jumped over the lazy dog and leaped over the tall fence in a beautiful garden filled with colorful flowers and bubbling fountains.', w2v_model)

import os
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
import nltk
from tqdm import tqdm  # Импортируем tqdm для отображения прогресса

# Укажите абсолютный путь к папке, в которой вы хотите создать файл
output_dir = '/content/drive/MyDrive/lab33/test-vector/'

# Проверьте, существует ли папка, и если нет, создайте её
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Загрузка или создание модели Word2Vec
w2v_model = Word2Vec.load('/content/drive/MyDrive/lab4/word2vec_model')

# Загрузка тестовой выборки
test_url = '/content/drive/MyDrive/lab1.1/ishodniki/train.csv'
column_names = ["class", "title", "text"]
df_test = pd.read_csv(test_url, names=column_names)

# Сконструируйте абсолютный путь к файлу test-embeddings.tsv
output_file_path = os.path.join(output_dir, 'train-embeddings.tsv')

# Функция для векторизации текста
def vectorize_text(text, w2v_model):
    text_vector = np.zeros(w2v_model.vector_size)  # Инициализируем нулевым вектором
    total_tokens = 0

    sentences = nltk.sent_tokenize(text)  # Разбиваем текст на предложения
    for sentence in sentences:
        tokens = nltk.word_tokenize(sentence)  # Токенизируем предложение
        for token in tokens:
            if token in w2v_model.wv:
                text_vector += w2v_model.wv[token]
                total_tokens += 1

    if total_tokens > 0:
        text_vector /= total_tokens  # Усредняем векторы слов

    return text_vector

# Откройте файл для записи
with open(output_file_path, 'w') as writefile:
    for index, row in tqdm(df_test.iterrows(), total=len(df_test)):  # Используем tqdm для отображения прогресса
        text = row['title'] + '. ' + row['text']
        doc_vector = vectorize_text(text, w2v_model)
        row_to_write = f"{index}"
        for embedding_component in doc_vector:
            row_to_write += f"\t{embedding_component}"
        writefile.write(row_to_write + '\n')
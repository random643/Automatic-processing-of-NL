# -*- coding: utf-8 -*-
"""lab2mysave goodfortrain.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11axQThvfWry_tumqH0LV2zVkUhek3s3v
"""

import csv
import re
from google.colab import drive
drive.mount('/content/drive')
import os

import nltk
from nltk.corpus import stopwords

nltk.download('punkt')
# скачиваем список стоп-слов
nltk.download('stopwords')

# загружаем список стоп-слов
stop_words = set(stopwords.words('english'))

from google.colab import drive
drive.mount('/content/drive')

# путь к папке с результатами
results_folder = '/content/drive/MyDrive/lab1.16/result/train'

# список словоформ для расчета n-грамм
result = []
for folder in os.listdir(results_folder):
    folder_path = os.path.join(results_folder, folder)
    if os.path.isdir(folder_path):
        # проходимся по всем файлам внутри папки
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            with open(file_path, 'r') as f:
                # читаем содержимое файла
                file_content = f.read()
                 # разделяем на строки
                lines = file_content.split('\n')
                # проходимся по каждой строке и добавляем токены из первого столбца в список
                for line in lines:
                    if line:
                        # Извлекаем только первый столбец (первое слово) из строки
                        token = line.split('\t')[0]
                        # удаляем знаки пунктуации
                        token = re.sub(r'[^\w\s]', '', token)
                        # приводим к нижнему регистру
                        token = token.lower()
                        # удаляем стоп-слова и символы
                        if token not in stop_words and token not in ['[', ']', '{', '}', ':', ';', '...', '', '-', '--']:
                            result.append(token)

import nltk
from nltk.collocations import TrigramCollocationFinder
from nltk.metrics import TrigramAssocMeasures
from nltk.corpus import stopwords
from nltk.util import ngrams

# Разбиваем текст на слова
words = ' '.join(result).split()

trigrams = ngrams(words, 3)
bigrams = ngrams(words, 2)
unigrams = ngrams(words, 1)

from collections import Counter
from tqdm import tqdm

unigram_counts = Counter(unigrams)
bigram_counts = Counter(bigrams)
trigram_counts = Counter(trigrams)

# Рассчитайте T-Score для каждой триграммы
t_scores = {}
for trigram in trigram_counts:
    n_iii = trigram_counts[trigram]
    n_ij = bigram_counts[(trigram[0], trigram[1])]
    n_jk = bigram_counts[(trigram[1], trigram[2])]
    n_k = unigram_counts[trigram[2]]
    n_j = unigram_counts[trigram[1]]

    #1e-10 в случае, если будет деление на ноль
    e_iii = (n_ij * n_jk * n_k) / ((n_j * n_k) + 1e-10)

    t_score = (n_iii - e_iii) / (n_iii ** 0.5)

    t_scores[trigram] = t_score

sorted_trigrams = sorted(trigram_counts.items(), key=lambda x: x[1], reverse=True)

for count, (trigram, freq) in enumerate(sorted_trigrams):
    print(f"Триграмма: {trigram}, Частота: {freq}, Scores: {t_scores[trigram]}")
    if count >= 20:
        break

# Создаем экземпляр TrigramCollocationFinder
finder = TrigramCollocationFinder.from_words(words)

# Подсчитываем частоту триграмм
trigram_freq = finder.ngram_fd


trigram_scores = finder.score_ngrams(TrigramAssocMeasures.student_t)

# Сортируем триграммы по частоте
sorted_trigrams = sorted(trigram_freq.items(), key=lambda x: x[1], reverse=True)


for count, (trigram, freq) in enumerate(sorted_trigrams):
    print(f"Триграмма: {trigram}, Частота: {freq}, Scores: {trigram_scores[count][1]}")
    if count >= 20:
        break